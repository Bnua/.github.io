<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[基于内容的语义感知系统]]></title>
    <url>%2F2019%2F07%2F04%2F1%2F</url>
    <content type="text"><![CDATA[1. 基于内容的推荐系统&emsp;&emsp;基于内容的推荐系统依赖物品和用户描述内容来构建其特征表示，然后基于这些特征表示推荐与目标用户曾明确表达过喜好的物品相类似的物品。该类推荐系统的基本过程是对目标用户属性(偏好，兴趣)与物品属性进行匹配，并返回目标用户在物品上的喜好程度。&emsp;&emsp;CRRS（基于内容的推荐系统）的高层次架构图如下，推荐的过程有三个阶段。每一个阶段由独立的部件控制。 内容分析器：当信息没有结构化时（如文本），某些预处理阶段需要抽取相关的结构化信息。这个部件的主要功能就是将来自信息源的对象（如文档、网页、新闻、产品描述等）的内容表示成恰当的格式，以便于下一阶段的处理。数据对象经过特征抽取技术的分析，目的是将原始信息空间转换到想要的物品描述格式（如将网页表示成关键词向量）。这样的描述格式作为信息学习器和过滤组件的输入。 信息学习器：这个模块收集了有关用户偏好的数据特征，并试图去泛化这些数据，从而构建用户特征信息。泛化策略通常是通过机器学习技术实现的，它可以从用户过去喜欢的或不喜欢的物品中推断出一个用户的兴趣模型。例如，网页推荐的信息学习器可以实现相关的反馈方法，通过学习技术将正负样例向量组合到一个表示用户特征的模型向量中。训练样本是由用户提供的具有正负反馈的网页。 过滤组件：这个模块将用户个人信息和物品在表示空间进行匹配，利用用户个人信息来推荐相关物品。这个组件的结果是一个二元的或者连续型的相关性判断（使用某种相似度来计算），后者能生成一个潜在感兴趣物品的排名清单。在上面提过的例子中，这种匹配是通过计算原型向量和物品向量的余弦相似度得到的。 &emsp;&emsp;推荐步骤的第一个阶段是由内容分析器完成的，它通常是借鉴了信息检索系统的技术。来自信息源的物品描述经过内容分析器，从非结构化的文本中抽取特征（如关键词、n-grams、概念等），从而得到结构化的物品描述，并储存在被表示物品库中。&emsp;&emsp;为了结构化和更新活跃用户Ua(必须为其提供推荐的用户)的个人信息，该用户对物品的偏好反应是通过某些渠道收集并记录在反馈库中的。这些被称作注释或反馈的相互作用和物品的相关描述一起被用在模型学习的过程中，这些信息对实际中预测新的相关物品的表示非常有用。因此，即使没有提供任何反馈，用户也可以清晰地定义他们自己感兴趣的领域作为初始的个人信息。&emsp;&emsp;通常情况下，我们能够区分这两种类型的相关性反馈：正面的信息（用户喜欢的特征）和负面的信息（用户不感兴趣的特征）。两种不同的技术都能用来记录用户的反馈。当系统要求用户明确评价物品时，这项技术通常称作“显式反馈”；反之，则称作“隐式反馈”，由于反馈来自监控和分析用户的行为，所以它不需要任何活跃用户的参与。确切的评价能够表明用户对一个物品相关或感兴趣的程度。主要有三种方式来得到显式的相关性反馈： 喜欢/不喜欢：利用一个简单的二元化评分刻度，将物品分成“相关的”或“不相关的”两大类。 评分：经常用来评价物品的一个离散的数值刻度，详情参见文献。当然，标记化的评价也可以映射到数值刻度，如在Syskill&amp;Webert中一样，把用户对网页的评价划分为热门、一般、冷门。 评论：收集并展示单一物品的评论给用户，使其成为用户加快决策过程的一种方式。例如，在Amazon或eBay上，用户的反馈可以帮助其他用户判断一件物品是否被大众所接受。文本评论是有益的，但是这些评论也会对用户造成负担，因为她必须阅读和理解每条评论，并决定哪些评论是正面的哪些是负面的，以及这些评论的程度。文献从情感计算研究领域中提出的先进技术，使得基于内容的推荐系统能够自动执行这种分析。 1. 基于内容的推荐系统基于内容的推荐与基于协同过滤的推荐相比有以下优点： 用户独立性：基于内容的推荐仅使用当前用户提供的评分来构建自己的个人信息。而协同过滤的方法需要其他用户的评分，来发现该用户最近的近邻，例如，由于对相同的物品评分相似而品味相似的用户。这时，只有当前用户最近邻很喜欢的物品才有可能推荐给当前用户。 透明度：通过显式地列出使得物品出现在推荐列表中的内容特征或描述，可以解释推荐系统是如何工作的。这些物品特征是决定是否信任该推荐的指标。相反，协同过滤系统是一个黑盒子，对一个推荐物品的唯一解释是相似品味的未知用户喜欢过该物品。 新物品：基于内容的推荐系统在没有任何用户评分的情况下也可以进行推荐。因此，新物品没有第一次评分会影响协同过滤推荐系统，因为协同过滤推荐系统仅依赖于用户的偏好产生推荐。所以只有当一个新物品被一系列用户评分之后，系统才可能推荐它。尽管如此，基于内容的推荐系统也有以下一些缺点： 可分析的内容有限：基于内容的推荐技术有一个天然的限制，即与推荐对象相关的特征数量和类型上的限制，不管是自动还是手动的。领域知识一般是必需的，例如，对于电影推荐，系统需要知道电影的演员、导演，有时候领域本体也是需要的。当分析的物品内容信息不足以区分哪些物品是用户喜欢的、哪些物品是用户不喜欢的时候，没有任何基于内容的推荐系统可以给出合适的推荐。有些解释只能获取物品内容的某些方面，但是还有很多别的方面也能影响用户体验。举个例子，在玩笑或者诗词里，没有足够的词频信息去为用户兴趣建模，这时，情感计算的技术就会更适用。此外，对于网页来说，文本特征抽取技术完全忽略其美学特征和附加的多媒体信息。总之，不论是手动还是自动为物品分配特征，都不足以定义物品不同的特点，而这些特点被证明对提取出用户兴趣是必要的。 过度特化：基于内容的推荐在本质上无法发现一些出人意料的物品。系统建议的物品和用户的个人信息高度匹配的时候，给用户的推荐也将会是与已有的评分物品相似的物品。这个缺点主要是由于基于内容的系统产生的推荐物品在新颖性上的缺陷，称作惊喜度问题。举例来说，当一个用户只评价了Stanley Kubrick导演的电影，那么她得到的推荐就只有这种类型的电影。一个“完美”的基于内容的技术可能很少发现任何新颖的东西，这限制了使用它的应用程序的范围。 新用户：在一个基于内容的推荐系统可以真正理解用户偏好且给出准确的推荐之前，需要收集足够的评分。因此，当只有很少的评分可用的时候，即对于新用户来说，系统不能提供可靠的推荐。接下来，将就采用何种策略来处理对上面提出的问题，进行介绍和讨论。更具体地，会阐述利用常识和特定领域的知识来提高内容解释的新技术。通过提供新的特征可能有助于克服传统的内容分析方式的限制，如 WordNet 或 Wikipedia 概念，帮助物品用一种更准确透明的方式进行推荐。此外，将推荐过程中用户定义词典，如大众分类，作为扩展词表加入考虑并进行整合的过程，使用惊喜度推荐，即新颖性很高的用户感兴趣的物品，来满足用户的可能方式将作为解决过度特化问题的解决方案进行分析。 3. 补充&emsp;&emsp;本篇文章只是大概写一下关于内容推荐的系统，具体的算法没有提及，日后有时间的话会不断的补充，最主要的一点是敲公式太麻烦了，此外还有基于情景感知的推荐系统以及基于约束的推荐系统日后应该不会提及，有兴趣的同学可以看一下《推荐系统：技术、评估及高效算法》，下一篇着重讲一下推荐系统中的数据挖掘方法。]]></content>
  </entry>
  <entry>
    <title><![CDATA[基于近邻推荐方法综述]]></title>
    <url>%2F2018%2F07%2F01%2F1%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;本文主要是对推荐系统中常用的常用算法的适用性以及优劣做一个总结，以让自己有一个更加清晰的认识，不会涉及公式以及算法的实现。 1 基于近邻推荐方法综述&emsp;&emsp;基于领域的推荐方法中主要包括了两种著名的推荐方法：基于用户的推荐和基于物品的推荐。在基于用户的推荐系统中，目标用户对某一物品的感兴趣程度是利用对该物品的历史评分，并且和目标用户有相似评分模式的其他用户来估计的。这里的目标用户的近邻是指与目标用户评分模式很一致的用户。基于物品的推荐系统，是根据某一用户对相似于目标物品的评分来预测该用户对目标物品的评分。在这种方法中，相似物品指那些被同一组用户评分并且评分值相近的物品。 2 基于近邻的两种推荐方法&emsp;&emsp;基于领域的推荐系统是根据相同的“口碑”的准则，根据和用户兴趣相同的人或者根据其他可信源来评价一个物品。 2.1 基于用户和基于物品的推荐方法的比较&emsp;&emsp;当需要选择是基于用户还是基于物品的推荐方法来实现推荐系统的时候，有5个准则需要考虑： 准确性 : 推荐系统的准确度很大程度依赖于系统中用户数和物品数之间的比例。通常，一小部分高可信度的用户要比一大部分相似度不是那么可信的近邻要合适的多，对于用户数量远远大于物品数量的大型商业系统，基于物品的推荐西永更加准确。同样对于用户数少与物品的推荐系统来说，可能采用基于用户的推荐系统更加准确。 效率：当用户数量远远大于物品数量时，基于物品推进方法在计算相似度权重方面所需的内存和时间要远远小于基于用户的方法，但是在线推荐阶段的时间复杂度因为只依赖于有效的物品和近邻数量的最大值，所以两者是相同的。 稳定性 稳定性主要取决于用户或者物品的改变频率和数量。 合理性 基于物品的推荐方法优点是容易证明推荐的合理性。基于用户的推荐系统就很难坐到这一点，因为用户不认识在在推荐结果中起到近邻作用的其他用户。 惊喜度 基于用户的推荐方法是根据用户的相似度来进行的，因此更有可能生成较为新颖的推荐结果，当推荐是基于很小部分近邻数更为有效。2.2 基于近邻方法的要素&emsp;&emsp;在近邻推荐系统中除了选择基于物品还是用户的推荐方法还存在一些重要属性比如:1)标准化评分;2)相似权重的计算;3)近邻的选择。2.2.1 评分标准化&emsp;&emsp;当一个用户对一个物品给予评分的时候，每个用户都有自己的评价准则。即使显示地定义每个评分的意义，有些用户依然不情愿给他们呢的物品评高分或者给他们不喜欢的物品评低分。均值中心化和Z-socre标准化可以将个人评分标准转换到更一般的整体评分标准。 均值中心化：通过与平均分的比较来决定一个评分为正或者负。 Z-score标准化：考虑个人评分范围不同带来的差异性。比如：用户 A 和用户 B 平均评分都是3，但是假设用户 A 的评分在1-5之间，用户 B 都是3。如果用户 B 给物品评5分，这会比用户 A 给物品5分更加以意外。因此反映了用户 B 更加喜欢这个物品。&emsp;&emsp;对比两者，Z-score在处理范围很大的离散评分或者连续值评分时更有优势。2.2.2 相似度权重的计算&emsp;&emsp;相似度权重计算在基于领域的推荐方法中扮演者重要的角色:1)可以选择可信的近邻用户用于评测评分；2）给予不同近邻在预测中的权重。计算相似度权重是基于近邻推荐系统中最重要的一个方面，他可以直接影响准确性和性能。具体的相似度计算方法本文略过，我们平常使用的余弦相似度扩展一下都可以。2.2.2 领域的选择2.2.2.1 领域的选择&emsp;&emsp;在大型的推荐系统中，由于硬件资源的限制，它不太可能存储所有的非零相似度。我们可以通过下边的方法来进行限制。 top-N过滤 阈值过滤 负值过滤2.2.2.2 用于预测的近邻&emsp;&emsp;一旦计算出每个用户或者物品的候选近邻列表，对一个新的评分预测可以通过k近邻方法得到，K 近邻也就是相似度权重对打的K个近邻。最重要的问题就是如何选择 K 值。&emsp;&emsp;K值的增加通常呈现出一个凸函数。因此当近邻数目限制一个很小的数的时候，预测度通常会低。当 K 很大的时候，一些重要的关联被一些不重要的关联所削弱。如何选择一个最优 K 值是我们优先考虑的问题。2.3 基于近邻的推荐方法优化&emsp;&emsp;基于近邻的推荐方法依然存在缺点： 覆盖受限：由于计算两个用户间的相似是基于他们对相同物品的评分，而且只有对相同物品进行评分的用户才可以作为近邻，所以覆盖受限。 对稀疏数据敏感：稀疏性是大多数推荐熊面临的共同问题，尤其面对冷启动问题，稀疏性更加严重。&emsp;&emsp;本文在这里列举两个方法，具体的方法和实现可以查阅相关文献: 基于图的方法 基于学习的方法]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[算法之路-常见字符串处理总结]]></title>
    <url>%2F2018%2F06%2F01%2F1%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;字符串处理是算法中比较重要的一部分。在本篇中，主要介绍一系列常见并且重要的字符串处理算法。 1.1 易位构词 定义&emsp;&emsp;如果对调字符，使得单词 w 变成单词 v，那么 w 就是 v 的易位构词。输入一个集合包含n个最大长度为 k 的单词，输出找到所有的易位构词。&emsp;&emsp;输入: &quot;le chien marche vers sa niche et trouve une limace de chine nue pleine de malice qui lui fait du charme&quot;&emsp;&emsp;输出: [&#39;chine&#39;, &#39;niche&#39;, &#39;chien&#39;], [&#39;marche&#39;, &#39;charme&#39;], [&#39;nue&#39;, &#39;une&#39;], [&#39;limace&#39;, &#39;malice&#39;] 算法思想&emsp;&emsp;思路比较简单，对每个单词进行排序，排序相同的放在一起。值得提醒的是，如果两个单词相同，则不是易构单词。 代码实现1234567891011121314def anagrams(w): w = list(set(w.split())) #分割删除重复项 d = &#123;&#125; for i in range (len(w)): #相同标签的单词序号保存在一起 s = "".join(sorted(w[i])) if s in d: d[s].append(i) else: d[s] = [i] reponse = [] for s in d: #输出易位单词 if len(d[s]) &gt; 1: reponse.append([w[i] for i in d[s]]) return reponse 1.2 KMP算法&emsp;&emsp;KMP算法可以说是一个很经典的算法，我们在日常编程中也经常会用到，KMP算法用来解决一系列字符串单模式匹配问题以及延伸出来的最大边KMP算法等等。 定义&emsp;&emsp;给定一个长度为 n 的字符串 s 和一个长度为 m 的待匹配模式字符串 t ，我们希望找到 t 在 s 中第一次出现的下标 i 。当 t 不是 s 的子串时，返回 -1。&emsp;&emsp;复杂度 O(n+m) 算法思想&emsp;&emsp;具体的算法思路可以参考这篇博客KMP算法详解。KMP算法主要是分为两个部分，第一部分计算模式字符串的 next 。第二部分计算字符串匹配。下边代码我们把两部分放在一个函数中。 代码实现1234567891011121314151617181920212223242526def kmp(s,p): len_s = len(s) len_p = len(p) next = [0] * len_p next[0] = -1 k = -1 j = 0 while j &lt; len_p-1: if k == -1 or p[j] == p[k]: j += 1 k += 1 next[j] = k else: k = next[k] i = 0 j = 0 while i &lt; len_s and j &lt; len_p: if j == -1 or s[i] == p[j]: i += 1 j += 1 else: j = next[j] if j == len_p: return i - j else: return -1 模式匹配算法除了KMP算法之外，还有Rabin-Karp算法，有时间的话会加上Rabin-Karp算法，Rabin-Karp算法时间复杂度一般也为 O(n) 。 1.3 回文字符： Manacher 算法&emsp;&emsp;manacher算法，我们习惯叫他 “马拉车”算法。&emsp;&emsp;Manacher算法的应用范围比较狭窄，但是它的思想和拓展kmp算法有很多共通之处，所以在这里介绍一下。Manacher算法是查找一个字符串的最长回文子串的线性算法。&emsp;&emsp;首先介绍一下什么是回文串，所谓回文串，简单来说就是正着读和反着读都是一样的字符串，比如abba，noon等等，一个字符串的最长回文子串即为这个字符串的子串中，是回文串的最长的那个。&emsp;&emsp;计算字符串的最长回文字串最简单的算法就是枚举该字符串的每一个子串，并且判断这个子串是否为回文串，这个算法的时间复杂度为O(n3)的，显然无法令人满意，稍微优化的一个算法是枚举回文串的中点，这里要分为两种情况，一种是回文串长度是奇数的情况，另一种是回文串长度是偶数的情况，枚举中点再判断是否是回文串，这样能把算法的时间复杂度降为O(n2)，但是当n比较大的时候仍然无法令人满意，Manacher算法可以在线性时间复杂度内求出一个字符串的最长回文字串。 定义&emsp;&emsp;如果字符串的第一个字符等于最后一个字符，而第二个字符又等于倒数第二个字符，以此类推，那么该字符就是一个回文字符串，“最长回文字串”就是要找到一个最长字串，其中字串是一个回文字串。&emsp;&emsp;输入:aaaabcdefgfedcbaa&emsp;&emsp;输出: aabcdefgfedcbaa 算法思想&emsp;&emsp;1.预处理字符串，将字符串处理为奇数，并且防止越界给字符串两边加上不同符号。&emsp;&emsp;2.对于位置 i 我们有三种情况处理，算法详情点击马拉车算法&emsp;&emsp;3.第三步就可以愉快的写代码。 代码实现12345678910111213141516def manachers(s): if s == &quot;&quot;: return (0,1) t = &quot;^#&quot; + &quot;#&quot;.join(s) + &quot;#$&quot; id_cent = 0 mx = 0 p = [0] * len(t) for i in range(1,len(t)-1): mirror = 2 * id_cent -i p[i] = max(0, min(p[mirror],mx-i)) while t[i + 1 + p[i]] == t[i - 1 - p[i]]: p[i] += 1 if i + p[i] &gt; mx: mx,id_cent, = i + p[i],i (k,i) = (max(p),p.index(max(p))) return (s[(i-k) // 2 :(i+k) //2]) 1.4小结&emsp;&emsp;字符串的处理我只记录了这三种，相对来说马拉车算法比较难理解。其他形式字符串的处理基本都是基于这些进行变化，比如:最长字串，最长公共字串等等。后续如果有时间，会继续在本文中更新Rabin-Karp算法。碰到有意思的字符串算法题也会继续更新。]]></content>
      <categories>
        <category>算法之路</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>python</tag>
        <tag>字符串</tag>
        <tag>易位构词</tag>
        <tag>KMP算法</tag>
        <tag>回文字符串</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐系统思维导图与架构]]></title>
    <url>%2F2018%2F05%2F06%2F1%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;本文主要是对《推荐系统技术 评估及高效算法》以及《推荐系统实战》两本书的总结，在后续的文章中会逐渐补充详细内容。 1. 推荐系统思维导图梳理 2. 推荐引擎架构&emsp;&emsp;推荐引擎使用一种或几种用户特征，按照一种推荐策略生成一种类型物品的推荐列表。 推荐系统引擎框架主要分为三个部分： A部分负责从数据库或者缓存中拿到用户行为数据，通过分析不同行为，生成当前用户的特征向量。不过如果是使用非行为特征，就不需要使用行为提取和分析模块了。该模块的输出是用户特征向量。 B部分负责将用户的特征向量通过特征-物品相关矩阵转化为初始推荐物品列表。 C部分负责对初始的推荐列表进行过滤、排名等处理，从而生成最终的推荐结果。 推荐引擎架构图 &emsp;2.1 生成用户特征向量&emsp;&emsp;一般来说，用户的特征包括两种，一种是用户的注册信息中可以提取出来的，主要包括用户的人口统计学特征。对于使用这种特征的推荐引擎，如果内存够，可以将存储这些特征的信息直接缓存在内存中，在推荐时直接拿到用户的特征数据并生成特征向量。除了这种特征，另一种特征主要是从用户的行为中计算出来的，本节着重讨论如何生成特征。&emsp;&emsp;一个特征向量由特征以及特征的权重组成，在利用用户行为计算特征向量时需要考虑以下因素。&emsp;&emsp;用户行为的种类 &emsp;在一个网站中，用户可以对物品产生很多不同种类的行为。用户可以浏览物品、单击物品的链接、收藏物品、给物品打分、购买物品、评论物品、给物品打上不同的标签、和好友分享物品、搜索不同的关键词等。这些行为都会对物品特征的权重产生影响，但不同行为的影响不同，大多时候很难确定什么行为更加重要，一般的标准就是用户付出代价越大的行为权重越高。比如，购买物品需要用户掏钱，所以用户一定会三思而后行，因此购买行为最为重要。相反，浏览物品的网页代价很小，所以这种行为对反映用户的真实兴趣的影响很小。&emsp;&emsp;用户行为产生的时间&emsp;一般来说，用户近期的行为比较重要，而用户很久之前的行为相对比较次要。因此，如果用户最近购买过某一个物品，那么这个物品对应的特征将会具有比较高的权重。&emsp;&emsp;用户行为的次数&emsp;有时用户对一个物品会产生很多次行为。比如用户会听一首歌很多次，看一部电视剧的很多集等。因此用户对同一个物品的同一种行为发生的次数也反映了用户对物品的兴趣，行为次数多的物品对应的特征权重越高。&emsp;&emsp;物品的热门程度&emsp;如果用户对一个很热门的物品产生了行为，往往不能代表用户的个性，因为用户可能是在跟风，可能对该物品并没有太大兴趣，特别是在用户对一个热门物品产生了偶尔几次不重要的行为（比如浏览行为）时，就更说明用户对这个物品可能没有什么兴趣，可能只是因为这个物品的链接到处都是，很容易点到而已。反之，如果用户对一个不热门的物品产生了行为，就说明了用户的个性需求。因此，推荐引擎在生成用户特征时会加重不热门物品对应的特征的权重。 &emsp;2.2 特征—物品相关推荐&emsp;&emsp;对于每个特征，我们可以在相关表中存储和它最相关的N个物品的ID。在线使用的特征-物品相关表一般都不止一张。以论文之间的相关表为例，计算论文之间的相关性既可以使用提出的协同过滤算法（即如果两篇论文的读者重合度很大说明两部电视剧相似），也可以通过内容计算（比如有相同的作者、关键词、相似的标题等）。即使是协同过滤，也可以根据不同的用户行为数据得到不同的相关表。比如可以根据用户的打分行为计算论文之间的相关性，也可以根据用户的浏览行为计算论文之间的相关性。总之，对于一个推荐引擎可以在配置文件中配置很多相关表以及它们的权重，而在线服务在启动时会将这些相关表按照配置的权重相加，然后将最终的相关表保存在内存中，而在给用户进行推荐时，用的已经是加权后的相关表了。&emsp;&emsp;从上面的架构图可以看到，特征—物品相关推荐模块还可以接受一个候选物品集合。候选物品集合的目的是保证推荐结果只包含候选物品集合中的物品。它的应用场合一般是产品需求希望将某些类型的电视剧推荐给用户。比如有些产品要求给用户推荐最近一周加入的新物品，那么候选物品集合就包括最近一周新加的物品。也许有读者会奇怪，为什么不在过滤模块中将候选集合外的电视剧过滤掉，而要在相关推荐模块中处理候选物品列表？这里举一个简单的例子说明原因。首先，一般来说对于协同过滤算法计算出的相关表，每个物品都会倾向于和比较热门的物品具有较高的相似度。那么假设用户购买过物品A，候选列表中包含了物品B，A和B相关，但A比B热门。那么，一般情况下，B在A的相关物品列表中会排在靠后的位置（假设排在第10名），而A在B的相关物品列表中会排在靠前的位置（假设排在第1名）。那么，如果推荐算法是给用户推荐和A最相关的5部电视剧，那么B就不会出现在用户的推荐列表中。但是，如果算法在给定候选列表时会用一种不同的方式进行推荐，比如如果用户看过和B最相关的5部电视剧中的某一部，就将B推荐给用户，那么这种情况下B就出现在推荐列表中了。&emsp;&emsp;一般来说，如果需要在一个小的候选物品集合中给用户推荐物品，那么可以考虑上述方法。但如果是要在一个很大的候选物品集合中给用户推荐物品，那么可以考虑直接在初始推荐列表中过滤掉不在候选物品集合中物品的方法。&emsp;&emsp;特征—物品相关推荐模块除了给用户返回物品推荐列表，还需要给推荐列表中的每个推荐结果产生一个解释列表，表明这个物品是因为哪些特征推荐出来的。 &emsp;2.3 过滤模块&emsp;&emsp;在得到初步的推荐列表后，还不能把这个列表展现给用户，首先需要按照产品需求对结果进行过滤，过滤掉那些不符合要求的物品。一般来说，过滤模块会过滤掉以下物品。&emsp;&emsp;用户已经产生过行为物品&emsp;因为推荐系统的目的是帮助用户发现物品，因此没必要给用户推荐他已经知道的物品，这样可以保证推荐结果的新颖性。&emsp;&emsp;候选物品以外的物品 &emsp;候选物品集合一般有两个来源，一个是产品需求。比如在首页可能要求将新加入的物品推荐给用户，因此需要在过滤模块中过滤掉不满足这一条件的物品。另一个来源是用户自己的选择，比如用户选择了某一个价格区间，只希望看到这个价格区间内的物品，那么过滤模块需要过滤掉不满足用户需求的物品。&emsp;&emsp;某些质量很差的物品 为了提高用户的体验，推荐系统需要给用户推荐质量好的物品，那么对于一些绝大多数用户评论都很差的物品，推荐系统需要过滤掉。这种过滤一般以用户的历史评分为依据，比如过滤掉平均分在2分以下的物品。 &emsp;2.4 排名模块&emsp;&emsp;经过过滤后的推荐结果直接展示给用户一般也没有问题，但如果对它们进行一些排名，则可以更好地提升用户满意度，一般排名模块需要包括很多不同的子模块，下面将对不同的模块分别加以介绍。 新颖性 多样性 时间多样性 用户反馈]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
        <tag>思维导图</tag>
      </tags>
  </entry>
</search>
